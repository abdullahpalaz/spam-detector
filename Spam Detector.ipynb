{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b370da7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Given headlines and their labels as real (0) or clickbait (1), it is asked from us to classify these headlines as real or clickbait. The classifier that is mandated is Naive Bayes Classifier. You will find the implementation of the Naive Bayes Classifier, as well as another classifier which does a good job given it's simplicity. Implementation details and comments are included both in the code and as markdown cells.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f94897e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\palaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\palaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\palaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import inflect\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcfb5685",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "672ab84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Clickbait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cuba Eliminates Mexico, 7-4, at Classic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This Is What Space Travel Really Looks Like</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which Mario Are You Based On Your Zodiac Sign</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>European oil companies stop trade with Iran</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This Dachshund Is Determined To Sleep With His...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28795</th>\n",
       "      <td>44 Perfect Songs To Listen To While You Write</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28796</th>\n",
       "      <td>Egypt announces Internet crime initiative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28797</th>\n",
       "      <td>Australian rules football: West Gippsland Latr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28798</th>\n",
       "      <td>Yankees Grab a Share of First Place as Everyon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28799</th>\n",
       "      <td>An Adorable Animals Advent Calendar: December 7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Headline  Clickbait\n",
       "0                Cuba Eliminates Mexico, 7-4, at Classic          0\n",
       "1            This Is What Space Travel Really Looks Like          1\n",
       "2          Which Mario Are You Based On Your Zodiac Sign          1\n",
       "3            European oil companies stop trade with Iran          0\n",
       "4      This Dachshund Is Determined To Sleep With His...          1\n",
       "...                                                  ...        ...\n",
       "28795      44 Perfect Songs To Listen To While You Write          1\n",
       "28796          Egypt announces Internet crime initiative          0\n",
       "28797  Australian rules football: West Gippsland Latr...          0\n",
       "28798  Yankees Grab a Share of First Place as Everyon...          0\n",
       "28799    An Adorable Animals Advent Calendar: December 7          1\n",
       "\n",
       "[28800 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecad139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class basic_classifier:\n",
    "    \n",
    "    #This classifier has two class variables:\n",
    "    #words_clickbait which is the is a dictionary of string keys and integer values. Keys there are words, preprocessed or not\n",
    "    #(check below for more information on that) and values are the frequencies. This is the dictionary for clickbait headlines.\n",
    "    #Words in this dictionary are from clickbait headlines.\n",
    "    \n",
    "    #words_real is the same thing with real headlines.\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.words_clickbait = {}\n",
    "        self.words_real = {}\n",
    "\n",
    "        for i, headline in enumerate(X):\n",
    "            index = X.index[i]\n",
    "            if y[index] == 0:\n",
    "                for word in headline:\n",
    "                    if word in self.words_real.keys():\n",
    "                        self.words_real[word] += headline.count(word)\n",
    "                    else:\n",
    "                        self.words_real[word] = headline.count(word)\n",
    "            elif y[index] == 1:\n",
    "                for word in headline:\n",
    "                    if word in self.words_clickbait.keys():\n",
    "                        self.words_clickbait[word] += headline.count(word)\n",
    "                    else:\n",
    "                        self.words_clickbait[word] = headline.count(word)\n",
    "    \n",
    "    #Get the frequency of word from respective dictionary.\n",
    "    #I used this for to be sure if the calculations were correct or not.\n",
    "    def get_word_frequency(self, word, clickbait):\n",
    "        if clickbait:\n",
    "            for key in self.words_clickbait:\n",
    "                if key == word:\n",
    "                    return self.words_clickbait.get(key)\n",
    "        else:\n",
    "            for key in self.words_real:\n",
    "                if key == word:\n",
    "                    return self.words_real.get(key)\n",
    "    \n",
    "    #To get useful words, I used these two functions below.\n",
    "    #It basically gets the most frequent words from both of the dictionaries.\n",
    "    def most_freq_words_priv(self, value_number, clickbait = False):\n",
    "        return_dict = {}\n",
    "        \n",
    "        if clickbait:\n",
    "            most_freqs = sorted(self.words_clickbait, key=self.words_clickbait.get, reverse=True)[:value_number]\n",
    "            for word in most_freqs:\n",
    "                return_dict[word] = self.get_word_frequency(word, True)\n",
    "            return return_dict\n",
    "        else: \n",
    "            most_freqs = sorted(self.words_real, key=self.words_real.get, reverse=True)[:value_number]\n",
    "            for word in most_freqs:\n",
    "                return_dict[word] = self.get_word_frequency(word, False)\n",
    "            return return_dict\n",
    "    \n",
    "    \n",
    "    def get_n_most_frequent_words(self, first_n):\n",
    "        freq_dict = {\n",
    "            \"clickbait\": {},\n",
    "            \"real\": {}\n",
    "        }\n",
    "        \n",
    "        freq_dict[\"clickbait\"] = self.most_freq_words_priv(first_n, True)\n",
    "        freq_dict[\"real\"] = self.most_freq_words_priv(first_n, False)\n",
    "        \n",
    "        return freq_dict\n",
    "    \n",
    "    #Same design with Assignment1 actually, just predict one then iterate over X_test to predict all instances.\n",
    "    \n",
    "    #Prediction is based on:\n",
    "    #for each word in the headline, if this word is more frequent on real headlines, then headline's num_real gets +1 point\n",
    "    #and if this word is more frequent on clickbait headlines, then headline's num_clickbait gets +1 point.\n",
    "    #Whichever of these num_real and num_clickbait is bigger, then that is the prediction of the headline.\n",
    "    def predict_one_headline(self, headline):  \n",
    "        num_clickbait = 0\n",
    "        num_real = 0\n",
    "        \n",
    "        for word in headline:\n",
    "            word = word.lower()\n",
    "            \n",
    "            clickbait_count = 0 if self.words_clickbait.get(word) == None else self.words_clickbait.get(word)\n",
    "            real_count = 0 if self.words_real.get(word) == None else  self.words_real.get(word)\n",
    "            \n",
    "            if real_count > clickbait_count:\n",
    "                num_real += 1\n",
    "            else:\n",
    "                num_clickbait += 1\n",
    "        \n",
    "        if num_clickbait < num_real:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        y_pred = []\n",
    "        for headline in X_test:\n",
    "            y_pred.append(self.predict_one_headline(headline))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a859050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, y_pred):\n",
    "    \n",
    "    true_count = 0\n",
    "    for index, el in enumerate(y):\n",
    "        if y_pred[index] == el:\n",
    "            true_count += 1\n",
    "    return 100 * true_count / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b976ae8",
   "metadata": {},
   "source": [
    "I added 100 factor later, so in the code below, you will see accuracy scores as e.g. 0.9. Sorry for the inconvinience but it take too long to run the whole notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec5d1a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_sentence(words):\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf20b19",
   "metadata": {},
   "source": [
    "### Basic prediction with no preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f42db0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Headline\"] = df[\"Headline\"].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a398a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"Headline\"]\n",
    "y = df[\"Clickbait\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b7a9ddf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier_basic = basic_classifier()\n",
    "classifier_basic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8adbafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier_basic.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96f12e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67.70833333333333"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ec0172",
   "metadata": {},
   "source": [
    "Wasn't expecting any better, since all I did here was tokenizing sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b0ffc83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clickbait': {'You': 2913,\n",
       "  'The': 2671,\n",
       "  'To': 1621,\n",
       "  'A': 1440,\n",
       "  'Your': 1374,\n",
       "  'Of': 1224,\n",
       "  \"'s\": 1177,\n",
       "  '``': 1146,\n",
       "  \"''\": 1145,\n",
       "  'Are': 917,\n",
       "  'In': 908,\n",
       "  'That': 851,\n",
       "  'This': 841,\n",
       "  'And': 781,\n",
       "  'Is': 767,\n",
       "  'On': 634,\n",
       "  'For': 617,\n",
       "  'What': 588,\n",
       "  'Will': 573,\n",
       "  'Things': 479},\n",
       " 'real': {'in': 2217,\n",
       "  ',': 1952,\n",
       "  'to': 1743,\n",
       "  'of': 1351,\n",
       "  'for': 756,\n",
       "  'the': 674,\n",
       "  'a': 629,\n",
       "  'on': 561,\n",
       "  'and': 550,\n",
       "  'at': 461,\n",
       "  \"'s\": 349,\n",
       "  ':': 345,\n",
       "  'US': 299,\n",
       "  '.': 283,\n",
       "  'New': 274,\n",
       "  'by': 254,\n",
       "  'after': 232,\n",
       "  \"'\": 220,\n",
       "  'as': 203,\n",
       "  'Is': 178}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_basic.get_n_most_frequent_words(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9be4ae8",
   "metadata": {},
   "source": [
    "Without removing stopwords:\n",
    "\n",
    "For clickbait headlines, \"You\", \"Your\" and \"What\" are the key words that make huge difference because in these headlines, these words are both often and semantically, these are the words that writers of these headlines use.\n",
    "\n",
    "For real headlines, \"US\", \"New\" and \"in\" are the words that make huge difference.\n",
    "\n",
    "The thing is, not only their frequencies, but frequency difference between real and clickbait headlines are important too. Because in calculation, this differs a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56055103",
   "metadata": {},
   "source": [
    "### Preprocessed data with basic classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e27e7dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8af0d349",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.apply(to_lowercase)\n",
    "X = X.apply(remove_punctuation)\n",
    "X = X.apply(replace_numbers)\n",
    "X = X.apply(remove_stopwords)\n",
    "X = X.apply(lemmatize_verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95da7cd1",
   "metadata": {},
   "source": [
    "The code is not here because I deleted it but, replacing numbers has a huge affect on classification. I guess it corrects lots of crooked part of the input data. Replacing numbers literally increased accuracy from 0.49 to 0.88."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8ceb11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To convert X to pd.Series, but may be unnecessary I don't know\n",
    "df[\"Headline\"] = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e592532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Clickbait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[cuba, eliminate, mexico, seventy-four, classic]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[space, travel, really, look, like]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[mario, base, zodiac, sign]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[european, oil, company, stop, trade, iran]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[dachshund, determine, sleep, teddy, bear]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28795</th>\n",
       "      <td>[forty-four, perfect, songs, listen, write]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28796</th>\n",
       "      <td>[egypt, announce, internet, crime, initiative]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28797</th>\n",
       "      <td>[australian, rule, football, west, gippsland, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28798</th>\n",
       "      <td>[yankees, grab, share, first, place, everyone,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28799</th>\n",
       "      <td>[adorable, animals, advent, calendar, december...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Headline  Clickbait\n",
       "0       [cuba, eliminate, mexico, seventy-four, classic]          0\n",
       "1                    [space, travel, really, look, like]          1\n",
       "2                            [mario, base, zodiac, sign]          1\n",
       "3            [european, oil, company, stop, trade, iran]          0\n",
       "4             [dachshund, determine, sleep, teddy, bear]          1\n",
       "...                                                  ...        ...\n",
       "28795        [forty-four, perfect, songs, listen, write]          1\n",
       "28796     [egypt, announce, internet, crime, initiative]          0\n",
       "28797  [australian, rule, football, west, gippsland, ...          0\n",
       "28798  [yankees, grab, share, first, place, everyone,...          0\n",
       "28799  [adorable, animals, advent, calendar, december...          1\n",
       "\n",
       "[28800 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98fa36f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"Headline\"]\n",
    "y = df[\"Clickbait\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "894dedd8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifier_basic_ = basic_classifier()\n",
    "classifier_basic_.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdce82db",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ = classifier_basic_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "782729ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.61111111111111"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_test, y_pred_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5101f1d2",
   "metadata": {},
   "source": [
    "Well, to be honest, just with preprocessing this is a great score for this classifier. I mean, this includes kind of a counting too it is not that sophisticated to get 0.91 accuracy. This is suprising.\n",
    "\n",
    "Here, one of the most important preprocessing part was replace_numbers function. I guess it just corrects too many things, because without it, I get like 0.47 accuracy socre from basic_classifier, and other preprocessing steps were included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d46cd865",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clickbait': {'things': 719,\n",
       "  'make': 706,\n",
       "  'people': 657,\n",
       "  'know': 591,\n",
       "  'time': 572,\n",
       "  'twenty-one': 455,\n",
       "  'seventeen': 453,\n",
       "  'actually': 403,\n",
       "  'base': 397,\n",
       "  'nt': 389,\n",
       "  'get': 389,\n",
       "  'nineteen': 380,\n",
       "  'two thousand and fifteen': 359,\n",
       "  'need': 347,\n",
       "  'like': 346,\n",
       "  'best': 328,\n",
       "  'look': 305,\n",
       "  'new': 295,\n",
       "  'life': 272,\n",
       "  'fifteen': 260},\n",
       " 'real': {'us': 785,\n",
       "  'new': 548,\n",
       "  'kill': 539,\n",
       "  'win': 311,\n",
       "  'die': 288,\n",
       "  'say': 283,\n",
       "  'two': 239,\n",
       "  'dead': 238,\n",
       "  'find': 227,\n",
       "  'uk': 223,\n",
       "  'president': 216,\n",
       "  'bomb': 203,\n",
       "  'obama': 194,\n",
       "  'crash': 190,\n",
       "  'first': 186,\n",
       "  'australian': 185,\n",
       "  'state': 176,\n",
       "  'world': 175,\n",
       "  'court': 171,\n",
       "  'plan': 171}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_basic_.get_n_most_frequent_words(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd45b81",
   "metadata": {},
   "source": [
    "When stopwords removed:\n",
    "\n",
    "For clickbait headlines \"things\", \"know\" and \"base\" are these huge impact words I guess. If you read clickbait headlines, these three words are almost are in every clickbait headline. It is suprising to see some numbers, but specific numbers, like 21, 17, 19, 15. I guess these are the most \"clickbait\" numbers. Just think about it, \"17 Things You Don't Know About Koalas!\". 2015 might be there because of the data, if the data is collected during 2015, then you may see things like \"What will your Zodiac sign be in 2015?\", I guess...\n",
    "\n",
    "For real headlines, country names make super sense. Since in general, these are news headlines, you expect to see lots of country names in a news headline. For specific three words I will choose \"us\", \"kill\", \"die\" because the world is not a good place..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5fcb2e",
   "metadata": {},
   "source": [
    "### Naive Bayes Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd86aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class naive_bayes:\n",
    "    \n",
    "    def __init__(self, alpha=0):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\"alpha\": self.alpha}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    #Priors, clickbait and real headline's word counts, and each word's count in clickbait and real headlines are calculated here.\n",
    "    #Inline lists are way and I mean WAY more faster than for loops and if statements so, I had to use inline lists.\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        #This is to avoid problems with index.\n",
    "        #Since y is a pandas.Series, when shuffled during train_test_split, indexes does not reset.\n",
    "        #But in calculations below, I used enumerate(X) so indexing of y there should be same with X for correct calculations.\n",
    "        y = y.reset_index()\n",
    "        y = y[\"Clickbait\"]\n",
    "        \n",
    "        \n",
    "        #Prior calculation\n",
    "        X = X + self.alpha #Laplace Smoothing with alpha\n",
    "        self.total_freq = np.sum(X)\n",
    "        self.prior_clickbait = np.sum([np.sum(X[i]) for i, x in enumerate(X) if y[i] == 1]) / self.total_freq\n",
    "        self.prior_real = np.sum([np.sum(X[i]) for i, x in enumerate(X) if y[i] == 0]) / self.total_freq\n",
    "        \n",
    "        #Word frequency calculation\n",
    "        d = [[document for index, document in enumerate(X) if y[index] == class_] for class_ in np.unique(y)]\n",
    "        self.word_freqs = [np.sum(d[class_], axis=0) for class_ in np.unique(y)]\n",
    "        \n",
    "        #Word counts in classes calculation\n",
    "        self.number_clickbait = np.sum(self.word_freqs[1])\n",
    "        self.number_real = np.sum(self.word_freqs[0])\n",
    "        \n",
    "        #THIS IS TOO SLOW\n",
    "        #DEPRECATED\n",
    "        #----------------\n",
    "        #Word frequency calculation\n",
    "        #self.word_freqs = np.ndarray(shape=(len(np.unique(y)), X.shape[1]), dtype=np.int64)\n",
    "        #for class_ in np.unique(y):\n",
    "        #    for i in range(X.shape[1]):\n",
    "        #        sum_ = 0\n",
    "        #        for j in np.arange(X.shape[0]):\n",
    "        #            if y[j] == class_:\n",
    "        #                sum_ += X[j][i]\n",
    "        #        self.word_freqs[class_][i] = sum_\n",
    "        #----------------\n",
    "        \n",
    "        self.y = y\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        y_pred = []\n",
    "        \n",
    "        for document in X_test:\n",
    "            pred_real = np.log(self.prior_real)\n",
    "            pred_clickbait = np.log(self.prior_clickbait)\n",
    "            for index, word_freq in enumerate(document):\n",
    "                if word_freq != 0:\n",
    "                    for i in range(word_freq):\n",
    "                        pred_real += np.log(self.word_freqs[0][index] / self.number_real)\n",
    "                        pred_clickbait += np.log(self.word_freqs[1][index] / self.number_clickbait)\n",
    "                        \n",
    "            if pred_real > pred_clickbait:\n",
    "                y_pred.append(0)\n",
    "            else:\n",
    "                y_pred.append(1)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    #This function is here for Part 3 of coding, to get most 10 influential words.\n",
    "    #However, it does not really work that way. It gives the most influential words, and there is an algorithm for that\n",
    "    #but I assumed that there should be more than 10 words which are higher than mean+standart_deviation but there is literally not.\n",
    "    #So this functions but poorly.\n",
    "    \n",
    "    #Returns indexes, words are given in turn_index_into_words function, which is out of the classifier.\n",
    "    def get_mosts(self, presence_coefficient, absence_coefficient):\n",
    "        \n",
    "        #This is done purely for my bad coding and probably too inefficient\n",
    "        word_freqs_ = [[el] for el in self.word_freqs]\n",
    "        \n",
    "        \n",
    "        mosts = {\"presence_real\": [],\n",
    "                \"absence_real\": [],\n",
    "                \"presence_clickbait\": [],\n",
    "                \"absence_clickbait\": []}\n",
    "        \n",
    "        #Differences is a list with following structure:\n",
    "        #[[real frequency - clickbait frequency of a word for word in self.word_freqs],\n",
    "        #[clickbait frequency - real frequency of a word for word in self.word_freqs]]\n",
    "        \n",
    "        #How it works:\n",
    "        #You get words with max freqs in their respective classes, and if the these words have difference (from differences list)\n",
    "        #that is higher than mean+presence_coefficient*standart_deviation, then this word is influential with it's presence.\n",
    "        #Vice versa for lower than mean-absence_coefficient*standart_deviation.\n",
    "        \n",
    "        #I do think this is a proper idea, but the functionality is not here, I think this is due to my bad maths.\n",
    "        #I mean, I used Gaussian Distribution as a model here, what can I do more?\n",
    "        differences = [word_freqs_[i][j] - word_freqs_[class_][j] for j in range(len(word_freqs_[0])) for i, class_ in enumerate(reversed(np.unique(self.y)))]\n",
    "        max_indexes = [np.where(word_freqs_[class_] == np.amax(word_freqs_[class_])) for class_ in np.unique(self.y)]\n",
    "        max_indexes = [max_indexes[i][1] for i in range(len(max_indexes))]\n",
    "        differences_stds = [np.std(x) for x in differences]\n",
    "        differences_means = [np.mean(x) for x in differences]\n",
    "        selected_indexes_presence = [[index for index in max_indexes[class_] if differences[class_][index] >= differences_means[class_]+presence_coefficient*differences_stds[class_]] for class_ in np.unique(self.y)]\n",
    "        selected_indexes_absence = [[index for index in max_indexes[class_] if differences[class_][index] <= differences_means[class_]-absence_coefficient*differences_stds[class_]] for class_ in np.unique(self.y)]\n",
    "        \n",
    "        mosts[\"presence_real\"] = selected_indexes_presence[0]\n",
    "        mosts[\"presence_clickbait\"] = selected_indexes_presence[1]\n",
    "        mosts[\"absence_real\"] = selected_indexes_absence[0]\n",
    "        mosts[\"absence_clickbait\"] = selected_indexes_absence[1]\n",
    "        \n",
    "        return mosts\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a8dea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As name suggests, turns the indexes we got from classifier.get_mosts to words of that vectorizer. Kinda slow tho.\n",
    "def turn_index_into_words(vectorizer, index_dict):\n",
    "    for key in index_dict:\n",
    "        index_dict[key] = [word for word in vectorizer.get_feature_names() for index in index_dict.get(key) if vectorizer.get_feature_names().index(word) == index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d73e2cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a257cb",
   "metadata": {},
   "source": [
    "### With stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25dc79ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Headline\"] = df[\"Headline\"].apply(word_tokenize)\n",
    "\n",
    "X = df[\"Headline\"]\n",
    "y = df[\"Clickbait\"]\n",
    "\n",
    "X = X.apply(to_lowercase)\n",
    "X = X.apply(remove_punctuation)\n",
    "X = X.apply(replace_numbers)\n",
    "X = X.apply(lemmatize_verbs)\n",
    "\n",
    "X = X.apply(tokens_to_sentence)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e4decb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1929b4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"Clickbait\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "277d4eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28800, 19079)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5ad1ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28800,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce8b2698",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "334307a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "276fa213",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"alpha\": [0.1, 0.01, 0.001, 0.0001, 0.00001]}\n",
    "\n",
    "scoring = [\"accuracy\"]\n",
    "grid_search = GridSearchCV(estimator=naive_bayes(), param_grid=parameters, cv=5, refit= \"accuracy\", scoring=scoring)\n",
    "\n",
    "grid_search = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63220cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  0.9713425925925927\n",
      "Best Estimator:  <__main__.naive_bayes object at 0x000001C125F8FF10>\n",
      "Best Parameters:  {'alpha': 1e-05}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Score: \",grid_search.best_score_)\n",
    "print(\"Best Estimator: \", grid_search.best_estimator_)\n",
    "print(\"Best Parameters: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10932d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.reset_index()\n",
    "y_test = y_test[\"Clickbait\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "edee0e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "7195    0\n",
       "7196    1\n",
       "7197    0\n",
       "7198    0\n",
       "7199    0\n",
       "Name: Clickbait, Length: 7200, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c0c29b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97      3599\n",
      "           1       0.97      0.98      0.97      3601\n",
      "\n",
      "    accuracy                           0.97      7200\n",
      "   macro avg       0.97      0.97      0.97      7200\n",
      "weighted avg       0.97      0.97      0.97      7200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "726f4578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3479  120]\n",
      " [  74 3527]]\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3fe39518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'presence_real': ['in'],\n",
       " 'absence_real': [],\n",
       " 'presence_clickbait': ['you'],\n",
       " 'absence_clickbait': []}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = grid_search.best_estimator_.get_mosts(1, -1)\n",
    "turn_index_into_words(vectorizer, words)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7483cdbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'presence_real': ['in'],\n",
       " 'absence_real': [],\n",
       " 'presence_clickbait': ['you'],\n",
       " 'absence_clickbait': []}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = grid_search.best_estimator_.get_mosts(5, -5)\n",
    "turn_index_into_words(vectorizer, words)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "007eff96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'presence_real': ['in'],\n",
       " 'absence_real': [],\n",
       " 'presence_clickbait': ['you'],\n",
       " 'absence_clickbait': []}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = grid_search.best_estimator_.get_mosts(10, -10)\n",
    "turn_index_into_words(vectorizer, words)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7953864",
   "metadata": {},
   "source": [
    "### With-out stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d26582c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"Headline\"]\n",
    "y = df[\"Clickbait\"]\n",
    "\n",
    "X = X.apply(to_lowercase)\n",
    "X = X.apply(remove_punctuation)\n",
    "X = X.apply(replace_numbers)\n",
    "X = X.apply(remove_stopwords)\n",
    "X = X.apply(lemmatize_verbs)\n",
    "\n",
    "X = X.apply(tokens_to_sentence)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "08d8e109",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d8dcc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.reset_index()\n",
    "y_test = y_test[\"Clickbait\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e4d1f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_stopwords = naive_bayes(0.00001)\n",
    "classifier_stopwords.fit(X_train, y_train)\n",
    "y_pred_stopwords = classifier_stopwords.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66fbf6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.88888888888889"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_test, y_pred_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a41c7d",
   "metadata": {},
   "source": [
    "Removing stopwords actually decreased accuracy. I guess this is because stopwords like \"you\", \"the\", \"your\" etc. do actually impact the classification. But this is a rare situation, obviously. In, general, removing stopwords should not lower the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f9d0589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      3599\n",
      "           1       0.96      0.96      0.96      3601\n",
      "\n",
      "    accuracy                           0.96      7200\n",
      "   macro avg       0.96      0.96      0.96      7200\n",
      "weighted avg       0.96      0.96      0.96      7200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0157998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3456  143]\n",
      " [ 153 3448]]\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred_stopwords)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dc1ef95f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'presence_real': ['us'],\n",
       " 'absence_real': [],\n",
       " 'presence_clickbait': ['twenty'],\n",
       " 'absence_clickbait': []}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_ = classifier_stopwords.get_mosts(1,-1)\n",
    "turn_index_into_words(vectorizer, words_)\n",
    "words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "469c3354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'presence_real': ['us'],\n",
       " 'absence_real': [],\n",
       " 'presence_clickbait': ['twenty'],\n",
       " 'absence_clickbait': []}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_ = classifier_stopwords.get_mosts(5,-5)\n",
    "turn_index_into_words(vectorizer, words_)\n",
    "words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "07b74754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'presence_real': ['us'],\n",
       " 'absence_real': [],\n",
       " 'presence_clickbait': ['twenty'],\n",
       " 'absence_clickbait': []}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_ = classifier_stopwords.get_mosts(10,-10)\n",
    "turn_index_into_words(vectorizer, words_)\n",
    "words_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45072c6e",
   "metadata": {},
   "source": [
    "So, changing coefficients does not do anything. Such shame. I couldn't get Part 3.1 done, but I think my idea is a good one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
